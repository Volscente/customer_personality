{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a8425c-154a-42b2-8e92-2b6c68dfcaba",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The notebook is intended to perform a binary classification over the 'Response' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f217cfae-abee-40ce-8e84-33c6ffc96f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Modules\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set Pandas Options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97637c10-2df0-42a5-b62d-e44b32f0cc2e",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722fbeee-0d35-456e-b2eb-1ca975bf123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('./../data/marketing_campaign_prepared.csv', encoding='latin1', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57b099-f6af-48e5-bd73-cde46dfe71a1",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4c6ac-994e-4b64-a414-616b0203e158",
   "metadata": {},
   "source": [
    "## Features & Label Definition\n",
    "\n",
    "The 'ID' column does not bring any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bd5e56e-960b-4a19-b6fc-703308de3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x and y\n",
    "X = data.drop(['ID', 'Response'], axis=1)\n",
    "y = data['Response']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089fd86-88fc-4206-a612-788601a7a8fe",
   "metadata": {},
   "source": [
    "## Train & Test Split\n",
    "\n",
    "Since the label is characterized by a strong imbalancing in the class distribution, we need to address it carefully:\n",
    "1. Ensure that the training and test sets have the same proportions of the two classes\n",
    "2. Oversample the minor class (i.e., randomly duplicate examples)\n",
    "3. Undersample the major class (i.e., randomly delete examples)\n",
    "4. Use several metrics (e.g., Accuracy, Precision, Recall, AUC)\n",
    "\n",
    "Use StratifiedShuffleSplit. This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.\n",
    "\n",
    "Note: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a986abb-2a63-4ae7-9333-7a4cfe292b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard train & test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2ff9b-1d55-4179-baef-ec46a29497fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the a Stratified K-fold Shuffle Splitter\n",
    "stratified_kfold = StratifiedShuffleSplit(n_splits=10,\n",
    "                                          test_size=.3, \n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f892767-69de-442b-bcb5-1be134ee12b9",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a14a67-8eba-41dd-9c58-4ac2dd190bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Features\n",
    "numerical_features = ['Year_Birth', \n",
    "                      'Income', \n",
    "                      'Recency', \n",
    "                      'MntWines', \n",
    "                      'MntFruits', \n",
    "                      'MntMeatProducts', \n",
    "                      'MntFishProducts', \n",
    "                      'MntSweetProducts', \n",
    "                      'MntGoldProds', \n",
    "                      'NumDealsPurchases', \n",
    "                      'NumWebPurchases', \n",
    "                      'NumCatalogPurchases', \n",
    "                      'NumStorePurchases', \n",
    "                      'NumWebVisitsMonth']\n",
    "\n",
    "# Categorical Text Features\n",
    "categorical_text_features = ['Education', \n",
    "                             'Marital_Status']\n",
    "\n",
    "# Categorical Numerical Features\n",
    "categorical_numerical_features = ['Kidhome', \n",
    "                                  'Teenhome', \n",
    "                                  'AcceptedCmp1', \n",
    "                                  'AcceptedCmp2', \n",
    "                                  'AcceptedCmp3', \n",
    "                                  'AcceptedCmp4', \n",
    "                                  'AcceptedCmp5', \n",
    "                                  'Complain', \n",
    "                                  'Dt_Customer_month', \n",
    "                                  'Dt_Customer_dayofweek']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03838419-b05d-4243-be09-3aa6582a269b",
   "metadata": {},
   "source": [
    "## Data Standardization\n",
    "\n",
    "Transform the individual features to look more or less like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "Keep in mind that tree-based methods are scale-invariant, so data standardization is not required.\n",
    "\n",
    "Standardization has to go after training-test split. That's because, standardizing the whole dataset and then split, would introduce into the training set some information about the mean and std of the test set. Remember to standardize the test set with the same scaler trained on the training set. This would be addressed by constructing a pipeline with the scaler as a step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a81e3ad1-409c-44d1-892b-b8a02761ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ColumnTransformer\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('numerical_transformation', StandardScaler(), numerical_features),\n",
    "    ('categorical_text_transformation', OrdinalEncoder(), categorical_text_features),\n",
    "    ('categorical_numerical_transformation', 'passthrough', categorical_numerical_features)\n",
    "], verbose_feature_names_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2436ba73-ed02-4d14-a0ee-176c21df6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('numerical_transformation', StandardScaler(),\n",
       "                                 ['Year_Birth', 'Income', 'Recency', 'MntWines',\n",
       "                                  'MntFruits', 'MntMeatProducts',\n",
       "                                  'MntFishProducts', 'MntSweetProducts',\n",
       "                                  'MntGoldProds', 'NumDealsPurchases',\n",
       "                                  'NumWebPurchases', 'NumCatalogPurchases',\n",
       "                                  'NumStorePurchases', 'NumWebVisitsMonth']),\n",
       "                                ('categorical_text_transformation',\n",
       "                                 OrdinalEncoder(),\n",
       "                                 ['Education', 'Marital_Status']),\n",
       "                                ('categorical_numerical_transformation',\n",
       "                                 'passthrough',\n",
       "                                 ['Kidhome', 'Teenhome', 'AcceptedCmp1',\n",
       "                                  'AcceptedCmp2', 'AcceptedCmp3',\n",
       "                                  'AcceptedCmp4', 'AcceptedCmp5', 'Complain',\n",
       "                                  'Dt_Customer_month',\n",
       "                                  'Dt_Customer_dayofweek'])])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the ColumnTransformer\n",
    "column_transformer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3d416bd-e59e-43d7-a3d5-841be93b9fb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Transformer categorical_text_transformation (type OrdinalEncoder) does not provide get_feature_names_out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcolumn_transformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/uc2_mobility/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:481\u001b[0m, in \u001b[0;36mColumnTransformer.get_feature_names_out\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m    479\u001b[0m transformer_with_feature_names_out \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, trans, column, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(fitted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 481\u001b[0m     feature_names_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_feature_name_out_for_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_features\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/uc2_mobility/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:446\u001b[0m, in \u001b[0;36mColumnTransformer._get_feature_name_out_for_transformer\u001b[0;34m(self, name, trans, column, feature_names_in)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# An actual transformer\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trans, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_feature_names_out\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(trans)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot provide get_feature_names_out.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     )\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(column, Iterable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m column\n\u001b[1;32m    452\u001b[0m ):\n\u001b[1;32m    453\u001b[0m     column \u001b[38;5;241m=\u001b[39m _safe_indexing(feature_names_in, column)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Transformer categorical_text_transformation (type OrdinalEncoder) does not provide get_feature_names_out."
     ]
    }
   ],
   "source": [
    "column_transformer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d2627fd-62f7-4eb0-8b1a-138c454cafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the columns\n",
    "X_train_transformed = column_transformer.transform(X_train)\n",
    "X_test_transformed = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f3cd41f-81f5-41a4-b9f6-7fe644c2c9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77014129, -0.87243071,  1.0682162 , ...,  0.        ,\n",
       "         6.        ,  6.        ],\n",
       "       [-0.32569585,  0.01578977, -1.6971568 , ...,  0.        ,\n",
       "         3.        ,  1.        ],\n",
       "       [ 0.11874959,  0.3553193 ,  1.48302215, ...,  0.        ,\n",
       "         8.        ,  2.        ],\n",
       "       ...,\n",
       "       [ 0.0298605 ,  0.6992879 ,  0.7916789 , ...,  0.        ,\n",
       "         7.        ,  0.        ],\n",
       "       [-0.41458494,  0.9057041 , -0.97124639, ...,  0.        ,\n",
       "         4.        ,  6.        ],\n",
       "       [-1.39236491,  0.39813285, -0.97124639, ...,  0.        ,\n",
       "         6.        ,  0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data transformation\n",
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde34e9-b215-44fe-be01-b8d6a82fd2d0",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15567a9-a6c1-416e-b76f-1e54dcb60d31",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "First benchmark model. Use standard train & test split and fit the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034d5af-cc3d-47ee-b360-b6eea8947347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model_lr = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "predictions_lr = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445728ea-04ad-4ef7-bd62-8a76cc61c5de",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with HYPEROPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72b12e-627c-47b6-96c2-e1fa1531f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hyperparamters space for Hyperopt\n",
    "hyperopt_parameters_space = {\n",
    "    'max_depth': hp.quniform(\"max_depth\", 3, 40, 2),\n",
    "    'gamma': hp.uniform ('gamma', 1, 15),\n",
    "    'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n",
    "    'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 0, 60, 3),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 3000, 10),\n",
    "    'seed': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926bca8-b0c3-4136-b7cf-55165266d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Objective Function\n",
    "def objective(space, column_transformer=column_transformer, cv=stratified_kfold, X=X, y=y, scoring='roc_auc'):\n",
    "    \n",
    "    # Create the estimator\n",
    "    clf=xgb.XGBClassifier(objective='binary:logistic',\n",
    "                          eval_metric='auc',\n",
    "                          n_estimators=int(space['n_estimators']), \n",
    "                          max_depth=int(space['max_depth']), \n",
    "                          gamma=space['gamma'],\n",
    "                          reg_alpha=int(space['reg_alpha']),\n",
    "                          min_child_weight=int(space['min_child_weight']),\n",
    "                          colsample_bytree=int(space['colsample_bytree']),\n",
    "                          use_label_encoder=False)\n",
    "    \n",
    "    # Define the Pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('feature_transformation', column_transformer), \n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "    # Init accuracy and precision list for K-fold\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    \n",
    "    # Train the model with K-fold\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "    \n",
    "        # Train the model\n",
    "        pipeline.fit(X[train_index], y[train_index])\n",
    "\n",
    "        # Get predicitons\n",
    "        predictions = pipeline.predict(X[test_index])\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy.append(accuracy_score(y[test_index], predictions))\n",
    "        \n",
    "        # Calculate precision\n",
    "        precision.append(precision_score(y[test_index], predictions))\n",
    "                                  \n",
    "        \n",
    "    # Calculate the score\n",
    "    cv_score = cross_val_score(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=1).mean()\n",
    "    accuracy_mean = accuracy.mean()\n",
    "    precision_score = precision.mean()\n",
    "    \n",
    "    print(cv_score)\n",
    "    print(accuracy_mean)\n",
    "    print(precision_score)\n",
    "                \n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f90003-c31f-4e11-a5f0-3c0fb4e209ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = fmin(fn=objective,\n",
    "                  space=hyperopt_parameters_space,\n",
    "                  algo=tpe.suggest,\n",
    "                  max_evals=50,\n",
    "                  trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7c5d7-ab43-42f1-a048-c2da36494240",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
